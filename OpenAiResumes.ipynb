{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b07ce1-8d66-4551-bf2a-9c5d0cfcfab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n",
      "ToolResourcesFileSearch(vector_store_ids=['vs_XPY9In8GPRHjeWbhVoqMfiPB'])\n",
      "\n",
      "assistant > file_search\n",
      "\n",
      "\n",
      "assistant > Score: 7 out of 10\n",
      "\n",
      "Summary: The candidate has a solid background in AI/ML research and DevOps, with over five years of experience, including leading a research team and implementing neural network quantization techniques. They possess relevant certifications in AWS and have experience with CI/CD pipelines, Docker, Kubernetes, and various AI/ML tools like TensorFlow and PyTorch. However, the resume lacks specific experience with some of the required technologies such as ChatGPT API, Azure, Snowflake, and Salesforce, and there is no mention of experience with Retrieval-Augmented Generation systems. The candidate demonstrates strong collaboration and problem-solving skills, which align with the role's requirements[0].\n",
      "[0] Resume.pdf\n",
      "AssistantDeleted(id='asst_HRhlBc0sVX2tb07XxW4A4YSo', deleted=True, object='assistant.deleted')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, dotenv_values \n",
    "# loading variables from .env file\n",
    "load_dotenv() \n",
    "\n",
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "assistantInstructions=\"\"\"You are a resume grading assistant, and your job is to assess resumes for the position of GenAI Production Engineer at Altimetrik. You will be provided with a resume file, and and you must take plenty of time to read the entire file that you are given. If you are unable to read a resume, you should just say \"I am unable to read the resume\". You should never give an assessment for a resume you have not read in its entirety.\n",
    "You will provide a score from 1 to 10, indicating how well the candidate fits this role. You will also give a short summary for why you have given this score.\n",
    "You must follow this output format exactly. Do not give other information or describe your process for evaluation:\n",
    "Score: [X out of 10]\n",
    "Summary: [A short summary]\n",
    "Use the following criteria to evaluate the candidate’s experience and skills against the job requirements, with a focus on both technical expertise and relevant industry experience.\n",
    "Evaluation Criteria:\n",
    "* Experience with AI/ML Production Systems: Does the candidate have at least 3 years of experience in software engineering with a specific focus on AI/ML project deployment? Look for experience with deploying or optimizing production-ready AI/ML systems.\n",
    "* Technical Skills and Cloud Expertise: Does the candidate have hands-on experience with the required cloud and AI technologies (ChatGPT API, AWS, Azure, Snowflake, and Salesforce)? Also, assess familiarity with serverless and microservices architectures.\n",
    "* Programming and DevOps Skills: Does the candidate demonstrate proficiency in Python, JavaScript, and TypeScript, as well as knowledge of DevOps tools (e.g., Git, Jenkins) and infrastructure as code (e.g., Terraform) and containerization tools (e.g., Docker, Kubernetes)?\n",
    "* Experience with Retrieval-Augmented Generation: Does the candidate have specific experience building or optimizing RAG systems, including designing metadata schemas or using vector embeddings for improved data retrieval?\n",
    "* Collaboration and Problem-Solving: Does the candidate’s experience show evidence of working in cross-functional teams and using strong problem-solving skills? Look for experience in agile environments and any collaborative projects in AI/ML contexts.\n",
    "* Career Growth Potential and Cultural Fit: Does the candidate exhibit qualities that align with Altimetrik's innovative and growth-focused culture? Assess their motivation, as indicated by previous experience or in their cover letter, for growth, innovation, and working collaboratively in a fast-paced tech environment.\"\"\"\n",
    "\n",
    "#create a new assistant\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Resume Evaluation Assistant\",\n",
    "    instructions=assistantInstructions,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")\n",
    "\n",
    "#select existing assistant\n",
    "#my_assistant = client.beta.assistants.retrieve(\"asst_GLnbL3Ff5vfNMlKihTk8elws\")\n",
    "\n",
    "\n",
    "# Create a vector store called \"GenAI Resumes\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"GenAI Resumes\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "filename=\"/Users/thomasgersic/Documents/Altimetrik/Recruiting/ResumesFromLinkedin/Resume.pdf\"\n",
    "file_paths = [filename]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)\n",
    "\n",
    "# Step 3: Update the assistant to use the new Vector Store\n",
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")\n",
    "\n",
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(filename, \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Evaluate this resume\",\n",
    "        # Attach the new file to the message.\n",
    "        \"attachments\": [\n",
    "            { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "        ],\n",
    "    }\n",
    "    ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)\n",
    "\n",
    "from typing_extensions import override\n",
    "from openai import AssistantEventHandler, OpenAI\n",
    " \n",
    "client = OpenAI()\n",
    "\n",
    "#Reference: https://platform.openai.com/docs/api-reference/assistants-streaming/events\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))\n",
    "        \n",
    "    @override\n",
    "    #reference: https://github.com/openai/openai-python/blob/main/helpers.md\n",
    "    def on_end(self):\n",
    "        #cleanup\n",
    "        response = client.beta.assistants.delete(assistant.id)\n",
    "        print(response)\n",
    "    \n",
    "\n",
    "\n",
    "# Then, we use the stream SDK helper\n",
    "# with the EventHandler class to create the Run\n",
    "# and stream the response.\n",
    "\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    event_handler=EventHandler(),\n",
    "    temperature=0.2\n",
    ") as stream:\n",
    "    stream.until_done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6aeb5-b6cd-4766-886b-43c4d161ae01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
